{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde9464d",
   "metadata": {},
   "source": [
    "\n",
    "# 00 — Project Overview & Setup (PDM: RUL + CoF)\n",
    "\n",
    "This notebook wires up the **Predictive Maintenance** project for two tasks:\n",
    "\n",
    "- **RUL Regression** (CNN, LSTM, XGBoost)\n",
    "- **Chance of Failure (CoF) Classification** (CNN, LSTM, XGBoost)\n",
    "\n",
    "It sets project paths, loads configs, verifies data availability, initializes a lightweight experiment registry, and builds a **data dictionary** preview for Line 10/20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521febcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports & Path Setup\n",
    "from pathlib import Path\n",
    "import sys, os, json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "# Project root = two levels up from this notebook if you keep the suggested structure\n",
    "# Fallback: current working directory\n",
    "NB_PATH = Path.cwd()\n",
    "ROOT = NB_PATH.parent if NB_PATH.name.lower() in {'rul','cof','notebooks'} else NB_PATH\n",
    "ROOT = ROOT if (ROOT / 'notebooks').exists() else NB_PATH\n",
    "\n",
    "DATA_DIR = ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "INTERIM_DIR = DATA_DIR / 'interim'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "CONFIGS_DIR = ROOT / 'configs'\n",
    "EXPERIMENTS_DIR = ROOT / 'experiments'\n",
    "SRC_DIR = ROOT / 'src'\n",
    "\n",
    "for d in [DATA_DIR, RAW_DIR, INTERIM_DIR, PROCESSED_DIR, CONFIGS_DIR, EXPERIMENTS_DIR, SRC_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Allow 'src' imports even if not installed as a package yet\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(f\"ROOT       : {ROOT}\")\n",
    "print(f\"DATA_DIR   : {DATA_DIR}\")\n",
    "print(f\"CONFIGS_DIR: {CONFIGS_DIR}\")\n",
    "print(f\"EXPERIMENTS: {EXPERIMENTS_DIR}\")\n",
    "print(f\"SRC        : {SRC_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d17918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Config helpers & defaults\n",
    "BASE_CFG = CONFIGS_DIR / 'base.yaml'\n",
    "TASK_RUL = CONFIGS_DIR / 'task_rul.yaml'\n",
    "TASK_COF = CONFIGS_DIR / 'task_cof.yaml'\n",
    "ALGO_CNN = CONFIGS_DIR / 'algo_cnn.yaml'\n",
    "ALGO_LSTM = CONFIGS_DIR / 'algo_lstm.yaml'\n",
    "ALGO_XGB = CONFIGS_DIR / 'algo_xgb.yaml'\n",
    "LINE10 = CONFIGS_DIR / 'line10.yaml'\n",
    "LINE20 = CONFIGS_DIR / 'line20.yaml'\n",
    "\n",
    "def ensure_yaml(path: Path, content: dict):\n",
    "    if not path.exists():\n",
    "        with path.open('w', encoding='utf-8') as f:\n",
    "            yaml.safe_dump(content, f, sort_keys=False)\n",
    "        print(f\"Created default config: {path.name}\")\n",
    "    else:\n",
    "        print(f\"Found config: {path.name}\")\n",
    "\n",
    "# Reasonable defaults — adjust as needed later\n",
    "ensure_yaml(BASE_CFG, {\n",
    "    'random_seed': 42,\n",
    "    'time_index': 'timestamp',\n",
    "    'id_col': 'machine_id',\n",
    "    'target_rul': 'RUL',\n",
    "    'target_cof': 'CoF',\n",
    "    'freq': '1min',  # resample frequency if needed\n",
    "})\n",
    "\n",
    "ensure_yaml(TASK_RUL, {\n",
    "    'task': 'RUL',\n",
    "    'metrics': ['rmse','mae','r2','nasa','silhouette'],\n",
    "    'train_val_test_split': {'method': 'time_series_split', 'n_splits': 3}\n",
    "})\n",
    "\n",
    "ensure_yaml(TASK_COF, {\n",
    "    'task': 'CoF',\n",
    "    'metrics': ['f1','recall','roc_auc'],\n",
    "    'train_val_test_split': {'method': 'time_series_split', 'n_splits': 3}\n",
    "})\n",
    "\n",
    "ensure_yaml(ALGO_CNN, {\n",
    "    'algo': 'cnn',\n",
    "    'epochs': 50,\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 1e-3,\n",
    "})\n",
    "\n",
    "ensure_yaml(ALGO_LSTM, {\n",
    "    'algo': 'lstm',\n",
    "    'epochs': 50,\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 1e-3,\n",
    "    'hidden_size': 64,\n",
    "    'num_layers': 2\n",
    "})\n",
    "\n",
    "ensure_yaml(ALGO_XGB, {\n",
    "    'algo': 'xgboost',\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9\n",
    "})\n",
    "\n",
    "ensure_yaml(LINE10, {'line': 10, 'source': 'csv', 'path': str(RAW_DIR / 'Line10' / 'DM_Machine_Learning_Line_10.csv')})\n",
    "ensure_yaml(LINE20, {'line': 20, 'source': 'csv', 'path': str(RAW_DIR / 'Line20' / 'DM_Machine_Learning_Line_20.csv')})\n",
    "\n",
    "def load_yaml(path: Path) -> dict:\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "base_cfg = load_yaml(BASE_CFG)\n",
    "print(\"Base config loaded:\", base_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a57f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Data discovery & preview\n",
    "# If a dataset exists in /mnt/data (as in chat attachment), place it under data/raw/Line10 for consistency.\n",
    "mnt_line10 = Path('/mnt/data/DM_Machine_Learning_Line_10.csv')\n",
    "proj_line10 = RAW_DIR / 'Line10' / 'DM_Machine_Learning_Line_10.csv'\n",
    "proj_line10.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if mnt_line10.exists():\n",
    "    try:\n",
    "        # Copy only if project copy doesn't exist yet\n",
    "        if not proj_line10.exists():\n",
    "            proj_line10.write_bytes(mnt_line10.read_bytes())\n",
    "            print(f\"Copied Line 10 CSV into project: {proj_line10}\")\n",
    "    except Exception as e:\n",
    "        print(\"Copy skipped, reason:\", e)\n",
    "\n",
    "# Attempt to load Line 10 (if present)\n",
    "df10 = None\n",
    "if proj_line10.exists():\n",
    "    try:\n",
    "        df10 = pd.read_csv(proj_line10)\n",
    "        print(f\"Loaded Line 10 shape: {df10.shape}\")\n",
    "        display(df10.head())\n",
    "    except Exception as e:\n",
    "        print(\"Failed to read Line 10 CSV:\", e)\n",
    "else:\n",
    "    print(\"Line 10 CSV not found yet — expected at:\", proj_line10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Build a quick data dictionary for any loaded frame\n",
    "def make_data_dictionary(df: pd.DataFrame, n_cat_top=10) -> pd.DataFrame:\n",
    "    info = []\n",
    "    for col in df.columns:\n",
    "        s = df[col]\n",
    "        dtype = str(s.dtype)\n",
    "        n_missing = int(s.isna().sum())\n",
    "        missing_pct = float(100 * n_missing / len(s)) if len(s) else 0.0\n",
    "        nunique = int(s.nunique(dropna=True))\n",
    "\n",
    "        entry = {\n",
    "            'column': col,\n",
    "            'dtype': dtype,\n",
    "            'n_missing': n_missing,\n",
    "            'missing_pct': round(missing_pct, 2),\n",
    "            'n_unique': nunique,\n",
    "        }\n",
    "\n",
    "        if np.issubdtype(s.dtype, np.number):\n",
    "            entry.update({\n",
    "                'min': s.min(skipna=True),\n",
    "                'max': s.max(skipna=True),\n",
    "                'mean': s.mean(skipna=True),\n",
    "                'std': s.std(skipna=True),\n",
    "            })\n",
    "        else:\n",
    "            # capture top categories for object columns\n",
    "            vc = s.value_counts(dropna=True).head(n_cat_top)\n",
    "            entry['top_values'] = \"; \".join([f\"{k}:{int(v)}\" for k, v in vc.items()])\n",
    "\n",
    "        info.append(entry)\n",
    "\n",
    "    return pd.DataFrame(info)\n",
    "\n",
    "if df10 is not None and not df10.empty:\n",
    "    dd10 = make_data_dictionary(df10)\n",
    "    # Save for reference\n",
    "    dd_path = PROCESSED_DIR / 'data_dictionary_line10.csv'\n",
    "    dd10.to_csv(dd_path, index=False)\n",
    "    print(f\"Data dictionary saved to: {dd_path}\")\n",
    "    try:\n",
    "        # Display nicely in the UI\n",
    "        from caas_jupyter_tools import display_dataframe_to_user\n",
    "        display_dataframe_to_user(\"Data Dictionary — Line 10\", dd10)\n",
    "    except Exception as e:\n",
    "        display(dd10.head(20))\n",
    "else:\n",
    "    print(\"No Line 10 dataframe available to profile.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Experiment registry bootstrap\n",
    "for task in ['RUL', 'CoF']:\n",
    "    task_dir = EXPERIMENTS_DIR / task\n",
    "    artifacts = task_dir / 'artifacts'\n",
    "    task_dir.mkdir(parents=True, exist_ok=True)\n",
    "    artifacts.mkdir(parents=True, exist_ok=True)\n",
    "    runs_csv = task_dir / 'runs.csv'\n",
    "    if not runs_csv.exists():\n",
    "        cols = [\n",
    "            'timestamp','task','algo','line','seed','params_json',\n",
    "            # RUL metrics\n",
    "            'rmse','mae','r2','nasa','silhouette',\n",
    "            # CoF metrics\n",
    "            'f1','recall','roc_auc',\n",
    "            # bookkeeping\n",
    "            'dataset_hash','notes','artifact_dir'\n",
    "        ]\n",
    "        pd.DataFrame(columns=cols).to_csv(runs_csv, index=False)\n",
    "        print(f\"Created experiment registry: {runs_csv}\")\n",
    "    else:\n",
    "        print(f\"Found experiment registry: {runs_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa3a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Utility to log experiment runs\n",
    "def log_run(task:str, algo:str, line:int, seed:int, params:dict,\n",
    "            metrics:dict, dataset_hash:str='', notes:str='') -> Path:\n",
    "    task_dir = EXPERIMENTS_DIR / task\n",
    "    artifacts_dir = task_dir / 'artifacts' / f\"{datetime.utcnow().strftime('%Y%m%dT%H%M%S')}_{algo}_L{line}_s{seed}\"\n",
    "    artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    runs_csv = task_dir / 'runs.csv'\n",
    "    row = {\n",
    "        'timestamp': datetime.utcnow().isoformat(),\n",
    "        'task': task, 'algo': algo, 'line': line, 'seed': seed,\n",
    "        'params_json': json.dumps(params, ensure_ascii=False),\n",
    "        'dataset_hash': dataset_hash, 'notes': notes,\n",
    "        'artifact_dir': str(artifacts_dir)\n",
    "    }\n",
    "    # Merge metrics keys that exist in schema\n",
    "    df_runs = pd.read_csv(runs_csv)\n",
    "    for k,v in metrics.items():\n",
    "        if k in df_runs.columns:\n",
    "            row[k] = v\n",
    "\n",
    "    df_runs = pd.concat([df_runs, pd.DataFrame([row])], ignore_index=True)\n",
    "    df_runs.to_csv(runs_csv, index=False)\n",
    "    print(f\"Logged run → {runs_csv.name}: {row['algo']} | {row['task']} | L{line} | seed={seed}\")\n",
    "    return artifacts_dir\n",
    "\n",
    "print(\"`log_run` ready. Import this cell into training notebooks (or paste function there).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306955fa",
   "metadata": {},
   "source": [
    "\n",
    "## Next steps (linked notebooks)\n",
    "\n",
    "- `RUL/01_eda_data_prep_RUL.ipynb` — unify time index, handle missingness, align sensors vs. OEE, build/verify **RUL labels**.\n",
    "- `RUL/02_feature_engineering_RUL.ipynb` — rolling stats, trend/lag, frequency features, normalization; snapshot to `data/processed`.\n",
    "- `RUL/03_train_eval_RUL.ipynb` (+ `03a/b/c`) — parameterized training for **CNN/LSTM/XGBoost**, log metrics to `experiments/RUL/runs.csv`.\n",
    "- `RUL/04_model_comparison_RUL.ipynb` — aggregate & visualize (RMSE/MAE/R²/NASA/Silhouette).\n",
    "\n",
    "We will mirror the flow for **CoF** under `notebooks/CoF/`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
