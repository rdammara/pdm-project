{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f041935",
   "metadata": {},
   "source": [
    "\n",
    "# 02 â€” Feature Engineering (RUL)\n",
    "\n",
    "- Load `data/processed/rul_labeled.parquet`\n",
    "- Create time-series features (lags, rolling stats, diffs, EMA)\n",
    "- Optional frequency-domain features (FFT energy, spectral centroid)\n",
    "- Train/Val/Test split indices (time-based)\n",
    "- Save `X_train.parquet`, `y_train.parquet`, etc. and a fitted scaler\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433191b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import joblib\n",
    "\n",
    "NB_PATH = Path.cwd()\n",
    "ROOT = NB_PATH.parents[1] if NB_PATH.name.lower() == 'rul' else NB_PATH\n",
    "DATA_DIR = ROOT / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "id_col = 'machine_id'     # keep in sync with configs\n",
    "time_col = 'timestamp'\n",
    "target = 'RUL'\n",
    "\n",
    "df = pd.read_parquet(PROCESSED_DIR / 'rul_labeled.parquet')\n",
    "df = df.sort_values([id_col, time_col]).reset_index(drop=True)\n",
    "print(\"Loaded:\", df.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Select numeric sensor columns (exclude id/time/line/target/failure flags)\n",
    "exclude = {id_col, time_col, target, '__line'}\n",
    "exclude |= {c for c in df.columns if c.lower() in {'failure','fail','is_failure','breakdown','fault'}}\n",
    "num_cols = [c for c in df.select_dtypes(include='number').columns if c not in exclude]\n",
    "print(\"Numeric sensor columns:\", len(num_cols))\n",
    "num_cols[:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Feature engineering helpers\n",
    "def add_lags(g, cols, lags=(1,2,3)):\n",
    "    for L in lags:\n",
    "        for c in cols:\n",
    "            g[f'{c}_lag{L}'] = g[c].shift(L)\n",
    "    return g\n",
    "\n",
    "def add_roll_stats(g, cols, windows=(3,5,15)):\n",
    "    for w in windows:\n",
    "        for c in cols:\n",
    "            g[f'{c}_roll{w}_mean'] = g[c].rolling(w).mean()\n",
    "            g[f'{c}_roll{w}_std']  = g[c].rolling(w).std()\n",
    "            g[f'{c}_roll{w}_min']  = g[c].rolling(w).min()\n",
    "            g[f'{c}_roll{w}_max']  = g[c].rolling(w).max()\n",
    "    return g\n",
    "\n",
    "def add_diffs(g, cols):\n",
    "    for c in cols:\n",
    "        g[f'{c}_diff1'] = g[c].diff(1)\n",
    "    return g\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599513be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Apply per machine to avoid leakage\n",
    "fe = []\n",
    "for gid, g in df.groupby(id_col):\n",
    "    g = g.copy()\n",
    "    g = add_lags(g, num_cols, lags=(1,2,3))\n",
    "    g = add_roll_stats(g, num_cols, windows=(3,5,15))\n",
    "    g = add_diffs(g, num_cols)\n",
    "    fe.append(g)\n",
    "\n",
    "df_fe = pd.concat(fe, ignore_index=True).dropna().reset_index(drop=True)\n",
    "print(\"After FE:\", df_fe.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecd59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Split by time using TimeSeriesSplit (global for simplicity)\n",
    "X = df_fe.drop(columns=[target])\n",
    "y = df_fe[target].values\n",
    "\n",
    "# scale numeric features (keep id/time separate)\n",
    "keep = [id_col, time_col, '__line'] if '__line' in X.columns else [id_col, time_col]\n",
    "X_num = X.drop(columns=keep, errors='ignore')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num.values)\n",
    "X_scaled = pd.DataFrame(X_num_scaled, columns=X_num.columns, index=X_num.index)\n",
    "X_scaled = pd.concat([X[keep], X_scaled], axis=1)\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, PROCESSED_DIR / 'rul_scaler.joblib')\n",
    "\n",
    "# TimeSeriesSplit to create indices; we will save the last split for train/val/test\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "splits = list(tscv.split(X_scaled))\n",
    "(train_idx, test_idx) = splits[-1]\n",
    "# split test again into val/test by half\n",
    "mid = (test_idx[0] + test_idx[-1]) // 2\n",
    "val_idx = np.arange(test_idx[0], mid+1)\n",
    "test_idx2 = np.arange(mid+1, test_idx[-1]+1)\n",
    "\n",
    "print(\"Index sizes -> train:\", len(train_idx), \"val:\", len(val_idx), \"test:\", len(test_idx2))\n",
    "\n",
    "# Save parquet datasets\n",
    "X_scaled.iloc[train_idx].to_parquet(PROCESSED_DIR / 'RUL_X_train.parquet', index=False)\n",
    "X_scaled.iloc[val_idx].to_parquet(PROCESSED_DIR / 'RUL_X_val.parquet', index=False)\n",
    "X_scaled.iloc[test_idx2].to_parquet(PROCESSED_DIR / 'RUL_X_test.parquet', index=False)\n",
    "\n",
    "pd.DataFrame({'RUL': y[train_idx]}).to_parquet(PROCESSED_DIR / 'RUL_y_train.parquet', index=False)\n",
    "pd.DataFrame({'RUL': y[val_idx]}).to_parquet(PROCESSED_DIR / 'RUL_y_val.parquet', index=False)\n",
    "pd.DataFrame({'RUL': y[test_idx2]}).to_parquet(PROCESSED_DIR / 'RUL_y_test.parquet', index=False)\n",
    "\n",
    "print(\"Saved processed train/val/test and scaler.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
