{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfeaf942",
   "metadata": {},
   "source": [
    "# 02 — Feature Engineering (RUL, Streaming)\n",
    "\n",
    "This notebook streams per-machine to keep memory low.\n",
    "- Input: `data/processed/rul_labeled.parquet`\n",
    "- Shards: `data/processed/shards_RUL`\n",
    "- Target: `RUL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a68de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import joblib\n",
    "\n",
    "NB_PATH = Path.cwd()\n",
    "ROOT = NB_PATH.parents[1] if NB_PATH.parts[-2].lower() in {'rul','cof'} else NB_PATH\n",
    "DATA_DIR = ROOT / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "print(\"ROOT:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config\n",
    "id_col   = 'machine_id'\n",
    "time_col = 'timestamp'\n",
    "target   = 'RUL'\n",
    "LABELED  = PROCESSED_DIR / 'rul_labeled.parquet'\n",
    "SHARDS_DIR = PROCESSED_DIR / 'shards_RUL'\n",
    "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('Using labeled file:', LABELED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load labeled dataset\n",
    "df = pd.read_parquet(LABELED)\n",
    "df = df.sort_values([id_col, time_col]).reset_index(drop=True)\n",
    "print(\"Loaded:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad0926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Column selection & downcast\n",
    "exclude = {id_col, time_col, '__line', target}\n",
    "exclude |= {c for c in df.columns if c.lower() in {'breakdown','failure','fail','is_failure'}}\n",
    "num_cols = [c for c in df.select_dtypes(include='number').columns if c not in exclude]\n",
    "print(\"Numeric sensors:\", len(num_cols))\n",
    "\n",
    "def downcast_numeric(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in g.select_dtypes(include='float').columns:\n",
    "        g[c] = pd.to_numeric(g[c], downcast='float')\n",
    "    for c in g.select_dtypes(include='integer').columns:\n",
    "        g[c] = pd.to_numeric(g[c], downcast='integer')\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Lightweight feature engineering\n",
    "def add_lags(g, cols, lags=(1,)):\n",
    "    for L in lags:\n",
    "        for c in cols:\n",
    "            g[f'{c}_lag{L}'] = g[c].shift(L)\n",
    "    return g\n",
    "\n",
    "def add_roll_stats(g, cols, windows=(3,5)):\n",
    "    for w in windows:\n",
    "        roll = g[cols].rolling(w, min_periods=w)\n",
    "        g[[f'{c}_roll{w}_mean' for c in cols]] = roll.mean().values\n",
    "        g[[f'{c}_roll{w}_std'  for c in cols]] = roll.std().values\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68356e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stream per machine → write shard to disk\n",
    "shard_paths = []\n",
    "for gid, g in df.groupby(id_col, sort=False):\n",
    "    g = g[[id_col, time_col, target] + num_cols].copy()\n",
    "    g = downcast_numeric(g)\n",
    "    g = add_lags(g, num_cols, lags=(1,))\n",
    "    g = add_roll_stats(g, num_cols, windows=(3,5))\n",
    "    g = g.dropna()\n",
    "\n",
    "    out_path = SHARDS_DIR / f\"part_{id_col}_{gid}.parquet\"\n",
    "    g.to_parquet(out_path, index=False)\n",
    "    shard_paths.append(out_path)\n",
    "\n",
    "print(f\"Shards written: {len(shard_paths)} → {SHARDS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fit scaler on a sample, then stream‑transform shards to splits\n",
    "scaler = StandardScaler()\n",
    "sample_rows = 100_000\n",
    "seen = 0\n",
    "X_buf = None\n",
    "for p in shard_paths:\n",
    "    g = pd.read_parquet(p)\n",
    "    feats = [c for c in g.columns if c not in [id_col, time_col, target]]\n",
    "    X_chunk = g[feats].values\n",
    "    X_buf = X_chunk if X_buf is None else np.vstack([X_buf, X_chunk])\n",
    "    seen += len(g)\n",
    "    if seen >= sample_rows:\n",
    "        break\n",
    "\n",
    "joblib.dump(scaler.fit(X_buf), PROCESSED_DIR / 'rul_scaler.joblib')\n",
    "print('Scaler fitted and saved:', PROCESSED_DIR / 'rul_scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a74ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build global time ordering to compute cutoffs for splits\n",
    "meta_parts = []\n",
    "for p in shard_paths:\n",
    "    g = pd.read_parquet(p, columns=[time_col])\n",
    "    meta_parts.append(g.assign(path=str(p)))\n",
    "meta = pd.concat(meta_parts, ignore_index=True).sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "splits = list(tscv.split(meta))\n",
    "train_idx, test_idx = splits[-1]\n",
    "mid = (test_idx[0] + test_idx[-1]) // 2\n",
    "val_idx = np.arange(test_idx[0], mid+1)\n",
    "test_idx2 = np.arange(mid+1, test_idx[-1]+1)\n",
    "\n",
    "cut_train_t = meta.iloc[train_idx][-1][time_col]\n",
    "cut_val_t   = meta.iloc[val_idx][-1][time_col]\n",
    "print('Cutoffs:', cut_train_t, '|', cut_val_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper to append parquet\n",
    "from pathlib import Path\n",
    "def append_parquet(df, path: Path):\n",
    "    if path.exists():\n",
    "        old = pd.read_parquet(path)\n",
    "        pd.concat([old, df], ignore_index=True).to_parquet(path, index=False)\n",
    "    else:\n",
    "        df.to_parquet(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ea0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stream shards → scale & route to split\n",
    "Xtr_path = PROCESSED_DIR / 'RUL_X_train.parquet'\n",
    "Xva_path = PROCESSED_DIR / 'RUL_X_val.parquet'\n",
    "Xte_path = PROCESSED_DIR / 'RUL_X_test.parquet'\n",
    "ytr_path = PROCESSED_DIR / 'RUL_y_train.parquet'\n",
    "yva_path = PROCESSED_DIR / 'RUL_y_val.parquet'\n",
    "yte_path = PROCESSED_DIR / 'RUL_y_test.parquet'\n",
    "\n",
    "for p in [Xtr_path, Xva_path, Xte_path, ytr_path, yva_path, yte_path]:\n",
    "    if p.exists():\n",
    "        p.unlink()\n",
    "\n",
    "for p in shard_paths:\n",
    "    g = pd.read_parquet(p)\n",
    "    feats = [c for c in g.columns if c not in [id_col, time_col, target]]\n",
    "    keep  = [id_col, time_col]\n",
    "    X_scaled = pd.DataFrame(scaler.transform(g[feats].values), columns=feats)\n",
    "    X_scaled = pd.concat([g[keep].reset_index(drop=True), X_scaled.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    mask_tr = g[time_col] <= cut_train_t\n",
    "    mask_va = (g[time_col] > cut_train_t) & (g[time_col] <= cut_val_t)\n",
    "    mask_te = g[time_col] > cut_val_t\n",
    "\n",
    "    append_parquet(X_scaled.loc[mask_tr], Xtr_path)\n",
    "    append_parquet(X_scaled.loc[mask_va], Xva_path)\n",
    "    append_parquet(X_scaled.loc[mask_te], Xte_path)\n",
    "\n",
    "    append_parquet(pd.DataFrame({target: g.loc[mask_tr, target].values}), ytr_path)\n",
    "    append_parquet(pd.DataFrame({target: g.loc[mask_va, target].values}), yva_path)\n",
    "    append_parquet(pd.DataFrame({target: g.loc[mask_te, target].values}), yte_path)\n",
    "\n",
    "print('Saved datasets:')\n",
    "print('  ', Xtr_path.name, Xva_path.name, Xte_path.name)\n",
    "print('  ', ytr_path.name, yva_path.name, yte_path.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
