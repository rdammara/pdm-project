{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f85ea7",
   "metadata": {},
   "source": [
    "# 02 — Feature Engineering for RUL (Combined Line 10 & 20)\n",
    "\n",
    "This notebook:\n",
    "- Loads the RUL-labeled dataset\n",
    "- Engineers time-series features per line (Line 10 and 20)\n",
    "- Uses sharding + union schema\n",
    "- Splits train/val/test per line using failure-aware time cutoffs\n",
    "- Saves combined RUL train/val/test splits to `data/processed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9ce673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: d:\\Documents\\Thesis\\pdm-project\n",
      "PROCESSED_DIR: d:\\Documents\\Thesis\\pdm-project\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "NB_PATH = Path.cwd()\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    cur = start\n",
    "    for _ in range(6):\n",
    "        if (cur / \"requirements.txt\").exists() or (cur / \"configs\").exists() or (cur / \"data\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    try:\n",
    "        i = [p.name.lower() for p in start.parents].index(\"notebooks\")\n",
    "        return start.parents[i+1]\n",
    "    except ValueError:\n",
    "        return start\n",
    "\n",
    "ROOT = find_project_root(NB_PATH)\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005eab5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (656762, 216) | From: d:\\Documents\\Thesis\\pdm-project\\data\\processed\\rul_labeled.parquet\n"
     ]
    }
   ],
   "source": [
    "id_col   = \"machine_id\"\n",
    "time_col = \"timestamp\"\n",
    "target   = \"RUL\"\n",
    "\n",
    "LABELED   = PROCESSED_DIR / \"rul_labeled.parquet\"\n",
    "CSV_FALLB = PROCESSED_DIR / \"rul_labeled.csv\"\n",
    "\n",
    "SHARDS_L10 = PROCESSED_DIR / \"shards_RUL_L10\"\n",
    "SHARDS_L20 = PROCESSED_DIR / \"shards_RUL_L20\"\n",
    "for p in [SHARDS_L10, SHARDS_L20]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not LABELED.exists() and not CSV_FALLB.exists():\n",
    "    raise FileNotFoundError(\"Missing labeled RUL file. Run 01_eda_data_prep_RUL.ipynb first.\")\n",
    "\n",
    "try:\n",
    "    if LABELED.exists():\n",
    "        df = pd.read_parquet(LABELED)\n",
    "        src = LABELED\n",
    "    else:\n",
    "        df = pd.read_csv(CSV_FALLB)\n",
    "        src = CSV_FALLB\n",
    "except Exception as e:\n",
    "    if \"pyarrow\" in str(e).lower() and CSV_FALLB.exists():\n",
    "        df = pd.read_csv(CSV_FALLB)\n",
    "        src = CSV_FALLB\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(\"Loaded:\", df.shape, \"| From:\", src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2d4cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machine_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>__line</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2025-01-01 00:00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>17520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2025-01-01 00:01:00</td>\n",
       "      <td>10</td>\n",
       "      <td>17519.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2025-01-01 00:02:00</td>\n",
       "      <td>10</td>\n",
       "      <td>17518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2025-01-01 00:03:00</td>\n",
       "      <td>10</td>\n",
       "      <td>17517.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2025-01-01 00:04:00</td>\n",
       "      <td>10</td>\n",
       "      <td>17516.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   machine_id           timestamp  __line      RUL\n",
       "0          10 2025-01-01 00:00:00      10  17520.0\n",
       "1          10 2025-01-01 00:01:00      10  17519.0\n",
       "2          10 2025-01-01 00:02:00      10  17518.0\n",
       "3          10 2025-01-01 00:03:00      10  17517.0\n",
       "4          10 2025-01-01 00:04:00      10  17516.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={\"Mesin\":\"machine_id\", \"Timestamp\":\"timestamp\"})\n",
    "df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
    "\n",
    "if \"__line\" not in df.columns:\n",
    "    if \"Line\" in df.columns:\n",
    "        df[\"__line\"] = df[\"Line\"]\n",
    "    else:\n",
    "        df[\"__line\"] = np.nan\n",
    "\n",
    "df[target] = pd.to_numeric(df[target], errors=\"coerce\")\n",
    "\n",
    "non_feature = {id_col, time_col, \"__line\", target, \"Breakdown\"}\n",
    "for c in df.columns:\n",
    "    if c not in non_feature and df[c].dtype == \"object\":\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values([id_col, time_col]).reset_index(drop=True)\n",
    "df[[id_col, time_col, \"__line\", target]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d12cca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAGS  = (1,)\n",
    "WINS  = (3,)\n",
    "BATCH = 32\n",
    "COVERAGE_MIN = 0.85\n",
    "MAX_FEATS     = 800\n",
    "\n",
    "def _raw_sensor_list(df_line, id_col, time_col, target):\n",
    "    skip = {id_col, time_col, \"__line\", target, \"Breakdown\"}\n",
    "    cand = [c for c in df_line.select_dtypes(include=\"number\").columns if c not in skip]\n",
    "    raw = [c for c in cand if (\"_lag\" not in c and \"_r\" not in c)]\n",
    "    return raw\n",
    "\n",
    "def _prune_by_coverage(df_line, feats, threshold=COVERAGE_MIN, limit=MAX_FEATS):\n",
    "    if not feats:\n",
    "        return []\n",
    "    cov = df_line[feats].notna().mean().sort_values(ascending=False)\n",
    "    keep = cov[cov >= threshold].index.tolist()\n",
    "    if not keep:\n",
    "        keep = cov.index.tolist()\n",
    "    if limit:\n",
    "        keep = keep[:limit]\n",
    "    return keep\n",
    "\n",
    "def _chunked(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "def engineer_per_line_to_shards(df_line: pd.DataFrame, out_dir: Path):\n",
    "    shard_paths = []\n",
    "\n",
    "    feats_base_raw = _raw_sensor_list(df_line, id_col, time_col, target)\n",
    "    feats_base = _prune_by_coverage(df_line, feats_base_raw, COVERAGE_MIN, MAX_FEATS)\n",
    "    print(f\"Raw sensors: {len(feats_base_raw)} | after pruning: {len(feats_base)}\")\n",
    "\n",
    "    for gid, g0 in df_line.groupby(id_col, sort=True):\n",
    "        g = g0[[id_col, time_col, \"__line\", target]].copy()\n",
    "        g[time_col] = pd.to_datetime(g[time_col], errors=\"coerce\")\n",
    "\n",
    "        for cols in _chunked(feats_base, BATCH):\n",
    "            block = g0[cols].astype(\"float32\").copy()\n",
    "            to_concat = [g]\n",
    "\n",
    "            for L in LAGS:\n",
    "                lagged = block.shift(L)\n",
    "                lagged.columns = [f\"{c}_lag{L}\" for c in cols]\n",
    "                to_concat.append(lagged)\n",
    "\n",
    "            for w in WINS:\n",
    "                roll_mean = block.rolling(w, min_periods=w).mean()\n",
    "                roll_mean.columns = [f\"{c}_r{w}_mean\" for c in cols]\n",
    "                to_concat.append(roll_mean)\n",
    "\n",
    "            g = pd.concat(to_concat, axis=1)\n",
    "            del block, to_concat\n",
    "            gc.collect()\n",
    "\n",
    "        warmup = max(LAGS or (0,)) + max(WINS or (1,)) - 1\n",
    "        if warmup > 0 and len(g) > warmup:\n",
    "            g = g.iloc[warmup:].copy()\n",
    "\n",
    "        feat_cols_all = [c for c in g.columns if c not in {id_col, time_col, \"__line\", target}]\n",
    "        if feat_cols_all:\n",
    "            g[feat_cols_all] = g[feat_cols_all].ffill().bfill()\n",
    "            g = g.dropna(subset=feat_cols_all, how=\"all\")\n",
    "\n",
    "        if len(g) == 0:\n",
    "            print(f\"⚠️ Empty after warm-up: machine={gid} — skipping\")\n",
    "            continue\n",
    "\n",
    "        out_path = out_dir / f\"part_{id_col}_{gid}.parquet\"\n",
    "        g.to_parquet(out_path, index=False)\n",
    "        shard_paths.append(out_path)\n",
    "        print(f\"✓ {out_path.name}: rows={len(g)}, feats={len(feat_cols_all)}\")\n",
    "\n",
    "        del g\n",
    "        gc.collect()\n",
    "\n",
    "    return shard_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e540f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw sensors: 211 | after pruning: 65\n",
      "✓ part_machine_id_10.parquet: rows=333598, feats=130\n",
      "Raw sensors: 211 | after pruning: 72\n",
      "✓ part_machine_id_20.parquet: rows=323158, feats=144\n",
      "Shards L10: 1\n",
      "Shards L20: 1\n"
     ]
    }
   ],
   "source": [
    "has_lines = df[\"__line\"].notna().any()\n",
    "paths_L10, paths_L20 = [], []\n",
    "\n",
    "if has_lines:\n",
    "    if (df[\"__line\"] == 10).any():\n",
    "        paths_L10 = engineer_per_line_to_shards(df[df[\"__line\"] == 10], SHARDS_L10)\n",
    "    if (df[\"__line\"] == 20).any():\n",
    "        paths_L20 = engineer_per_line_to_shards(df[df[\"__line\"] == 20], SHARDS_L20)\n",
    "else:\n",
    "    df[\"__line\"] = 10\n",
    "    paths_L10 = engineer_per_line_to_shards(df, SHARDS_L10)\n",
    "\n",
    "print(\"Shards L10:\", len(paths_L10))\n",
    "print(\"Shards L20:\", len(paths_L20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "505450c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union features: 272\n"
     ]
    }
   ],
   "source": [
    "ignore = {id_col, time_col, \"__line\", target}\n",
    "\n",
    "def collect_feats(paths, k=5):\n",
    "    feats = set()\n",
    "    for p in sorted(paths)[:k]:\n",
    "        g = pd.read_parquet(p)\n",
    "        feats |= set([c for c in g.columns if c not in ignore])\n",
    "    return feats\n",
    "\n",
    "feat10 = collect_feats(paths_L10) if paths_L10 else set()\n",
    "feat20 = collect_feats(paths_L20) if paths_L20 else set()\n",
    "ALL_FEATS = sorted(feat10 | feat20)\n",
    "print(\"Union features:\", len(ALL_FEATS))\n",
    "\n",
    "def align_union(g: pd.DataFrame, all_feats, target: str) -> pd.DataFrame:\n",
    "    g = g.copy()\n",
    "    for c in all_feats:\n",
    "        if c not in g.columns:\n",
    "            g[c] = pd.NA\n",
    "    cols = [id_col, time_col, \"__line\"] + all_feats + [target]\n",
    "    return g[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c2e5f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L10 cutoffs: 2025-08-16 04:05:00 | 2025-08-18 06:03:00 | {'mode': 'failure_quantiles', 'total_pos': 10410}\n",
      "L20 cutoffs: 2025-08-13 08:46:00 | 2025-08-13 09:23:00 | {'mode': 'failure_quantiles', 'total_pos': 248}\n"
     ]
    }
   ],
   "source": [
    "def cutoffs_failure_aware_rul(paths, time_col=\"timestamp\", target=\"RUL\",\n",
    "                              frac_train=0.70, frac_val=0.85, min_pos=10):\n",
    "    if not paths:\n",
    "        return None, None, {\"mode\":\"no_paths\"}\n",
    "\n",
    "    pos_ts = []\n",
    "    all_min, all_max = None, None\n",
    "    total_pos = 0\n",
    "\n",
    "    for p in paths:\n",
    "        g = pd.read_parquet(p, columns=[time_col, target])\n",
    "        g[time_col] = pd.to_datetime(g[time_col], errors=\"coerce\")\n",
    "        g = g.dropna(subset=[time_col, target])\n",
    "\n",
    "        if not g.empty:\n",
    "            all_min = g[time_col].min() if all_min is None else min(all_min, g[time_col].min())\n",
    "            all_max = g[time_col].max() if all_max is None else max(all_max, g[time_col].max())\n",
    "\n",
    "        mask_fail = g[target] <= 1.0\n",
    "        npos = int(mask_fail.sum())\n",
    "        total_pos += npos\n",
    "        if npos:\n",
    "            pos_ts.append(g.loc[mask_fail, time_col])\n",
    "\n",
    "    if total_pos < min_pos:\n",
    "        meta = []\n",
    "        for p in paths:\n",
    "            gg = pd.read_parquet(p, columns=[time_col])\n",
    "            gg[time_col] = pd.to_datetime(gg[time_col], errors=\"coerce\")\n",
    "            meta.append(gg)\n",
    "        meta = pd.concat(meta, ignore_index=True).dropna(subset=[time_col]).sort_values(time_col)\n",
    "        if meta.empty:\n",
    "            return None, None, {\"mode\":\"empty_time_meta\"}\n",
    "        q1 = meta[time_col].quantile(frac_train)\n",
    "        q2 = meta[time_col].quantile(frac_val)\n",
    "        return q1, q2, {\"mode\":\"time_quantiles_fallback\", \"total_pos\": total_pos}\n",
    "\n",
    "    pos_ts = pd.concat(pos_ts, ignore_index=True).sort_values()\n",
    "    cut_train = pos_ts.quantile(frac_train, interpolation=\"nearest\")\n",
    "    cut_val   = pos_ts.quantile(frac_val,   interpolation=\"nearest\")\n",
    "\n",
    "    cut_train = pd.to_datetime(cut_train)\n",
    "    cut_val   = pd.to_datetime(cut_val)\n",
    "\n",
    "    if cut_train >= cut_val:\n",
    "        uniq = pos_ts.drop_duplicates().reset_index(drop=True)\n",
    "        idx_train = int(uniq.searchsorted(cut_train, side=\"right\") - 1)\n",
    "        idx_val   = min(idx_train + 1, len(uniq) - 1)\n",
    "        cut_val   = uniq.iloc[idx_val]\n",
    "\n",
    "    if all_min is not None:\n",
    "        cut_train = max(cut_train, all_min)\n",
    "        cut_val   = max(cut_val,   all_min)\n",
    "    if all_max is not None:\n",
    "        cut_train = min(cut_train, all_max)\n",
    "        cut_val   = min(cut_val, all_max)\n",
    "\n",
    "    return cut_train, cut_val, {\"mode\":\"failure_quantiles\", \"total_pos\": total_pos}\n",
    "\n",
    "cut10_tr, cut10_va, info10 = cutoffs_failure_aware_rul(paths_L10, time_col=time_col, target=target)\n",
    "cut20_tr, cut20_va, info20 = cutoffs_failure_aware_rul(paths_L20, time_col=time_col, target=target)\n",
    "print(\"L10 cutoffs:\", cut10_tr, \"|\", cut10_va, \"|\", info10)\n",
    "print(\"L20 cutoffs:\", cut20_tr, \"|\", cut20_va, \"|\", info20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92009035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def append_parquet(df, path: Path):\n",
    "    if path.exists():\n",
    "        old = pd.read_parquet(path)\n",
    "        pd.concat([old, df], ignore_index=True).to_parquet(path, index=False)\n",
    "    else:\n",
    "        df.to_parquet(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "996c074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\614702887.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-line RUL splits written.\n"
     ]
    }
   ],
   "source": [
    "Xtr_L10 = PROCESSED_DIR / \"RUL_X_train_L10.parquet\"\n",
    "Xva_L10 = PROCESSED_DIR / \"RUL_X_val_L10.parquet\"\n",
    "Xte_L10 = PROCESSED_DIR / \"RUL_X_test_L10.parquet\"\n",
    "ytr_L10 = PROCESSED_DIR / \"RUL_y_train_L10.parquet\"\n",
    "yva_L10 = PROCESSED_DIR / \"RUL_y_val_L10.parquet\"\n",
    "yte_L10 = PROCESSED_DIR / \"RUL_y_test_L10.parquet\"\n",
    "\n",
    "Xtr_L20 = PROCESSED_DIR / \"RUL_X_train_L20.parquet\"\n",
    "Xva_L20 = PROCESSED_DIR / \"RUL_X_val_L20.parquet\"\n",
    "Xte_L20 = PROCESSED_DIR / \"RUL_X_test_L20.parquet\"\n",
    "ytr_L20 = PROCESSED_DIR / \"RUL_y_train_L20.parquet\"\n",
    "yva_L20 = PROCESSED_DIR / \"RUL_y_val_L20.parquet\"\n",
    "yte_L20 = PROCESSED_DIR / \"RUL_y_test_L20.parquet\"\n",
    "\n",
    "for p in [Xtr_L10, Xva_L10, Xte_L10, ytr_L10, yva_L10, yte_L10,\n",
    "          Xtr_L20, Xva_L20, Xte_L20, ytr_L20, yva_L20, yte_L20]:\n",
    "    if p.exists():\n",
    "        p.unlink()\n",
    "\n",
    "def route_and_write(paths, cut_tr, cut_va, tag):\n",
    "    if not paths or cut_tr is None or cut_va is None:\n",
    "        print(f\"Skipping line {tag}: missing paths or cutoffs\")\n",
    "        return\n",
    "\n",
    "    for p in paths:\n",
    "        g = pd.read_parquet(p)\n",
    "        g = align_union(g, ALL_FEATS, target)\n",
    "\n",
    "        mask_tr = g[time_col] <= cut_tr\n",
    "        mask_va = (g[time_col] > cut_tr) & (g[time_col] <= cut_va)\n",
    "        mask_te = g[time_col] > cut_va\n",
    "\n",
    "        base_cols = [id_col, time_col, \"__line\"] + ALL_FEATS\n",
    "        X_tr = g.loc[mask_tr, base_cols]\n",
    "        X_va = g.loc[mask_va, base_cols]\n",
    "        X_te = g.loc[mask_te, base_cols]\n",
    "\n",
    "        y_tr = pd.DataFrame({target: g.loc[mask_tr, target].values})\n",
    "        y_va = pd.DataFrame({target: g.loc[mask_va, target].values})\n",
    "        y_te = pd.DataFrame({target: g.loc[mask_te, target].values})\n",
    "\n",
    "        if tag == \"L10\":\n",
    "            append_parquet(X_tr, Xtr_L10); append_parquet(X_va, Xva_L10); append_parquet(X_te, Xte_L10)\n",
    "            append_parquet(y_tr, ytr_L10); append_parquet(y_va, yva_L10); append_parquet(y_te, yte_L10)\n",
    "        else:\n",
    "            append_parquet(X_tr, Xtr_L20); append_parquet(X_va, Xva_L20); append_parquet(X_te, Xte_L20)\n",
    "            append_parquet(y_tr, ytr_L20); append_parquet(y_va, yva_L20); append_parquet(y_te, yte_L20)\n",
    "\n",
    "route_and_write(paths_L10, cut10_tr, cut10_va, \"L10\")\n",
    "route_and_write(paths_L20, cut20_tr, cut20_va, \"L20\")\n",
    "print(\"Per-line RUL splits written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec251fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\3814975472.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat(dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 2 → RUL_X_train.parquet (650207 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\3814975472.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat(dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 2 → RUL_X_val.parquet (3035 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard Dammara\\AppData\\Local\\Temp\\ipykernel_9188\\3814975472.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat(dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 2 → RUL_X_test.parquet (3514 rows)\n",
      "Merged 2 → RUL_y_train.parquet (650207 rows)\n",
      "Merged 2 → RUL_y_val.parquet (3035 rows)\n",
      "Merged 2 → RUL_y_test.parquet (3514 rows)\n"
     ]
    }
   ],
   "source": [
    "def concat_splits(base_name):\n",
    "    files = list(PROCESSED_DIR.glob(f\"{base_name}_L*.parquet\"))\n",
    "    if not files:\n",
    "        return None\n",
    "    dfs = [pd.read_parquet(f) for f in files if f.exists()]\n",
    "    if not dfs:\n",
    "        return None\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "    out_path = PROCESSED_DIR / f\"{base_name}.parquet\"\n",
    "    out.to_parquet(out_path, index=False)\n",
    "    print(f\"Merged {len(dfs)} → {out_path.name} ({len(out)} rows)\")\n",
    "    return out_path\n",
    "\n",
    "for nm in [\"RUL_X_train\", \"RUL_X_val\", \"RUL_X_test\",\n",
    "           \"RUL_y_train\", \"RUL_y_val\", \"RUL_y_test\"]:\n",
    "    concat_splits(nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b399789f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUL_y_train rows: 650207 | RUL min: 0.0 | RUL max: 297119.0\n",
      "RUL_y_val rows: 3035 | RUL min: 0.0 | RUL max: 959.0\n",
      "RUL_y_test rows: 3514 | RUL min: 0.0 | RUL max: 959.0\n"
     ]
    }
   ],
   "source": [
    "for nm in [\"RUL_y_train\", \"RUL_y_val\", \"RUL_y_test\"]:\n",
    "    p = PROCESSED_DIR / f\"{nm}.parquet\"\n",
    "    if p.exists():\n",
    "        y = pd.read_parquet(p)\n",
    "        print(nm, \"rows:\", len(y),\n",
    "              \"| RUL min:\", float(y[target].min()),\n",
    "              \"| RUL max:\", float(y[target].max()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
