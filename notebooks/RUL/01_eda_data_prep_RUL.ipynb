{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b1cd5a",
   "metadata": {},
   "source": [
    "\n",
    "# 01 â€” EDA & Data Prep (RUL)\n",
    "\n",
    "Goals:\n",
    "- Load Line 10/20 raw data (CSV/SQL as configured)\n",
    "- Parse timestamps, sort, align by machine_id\n",
    "- Basic EDA (missingness, ranges, sampling frequency)\n",
    "- Build **RUL labels** per record\n",
    "- Save a clean, labeled snapshot to `data/processed/`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392def88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import sys, json, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from datetime import timedelta\n",
    "\n",
    "# Paths\n",
    "NB_PATH = Path.cwd()\n",
    "ROOT = NB_PATH.parents[1] if NB_PATH.name.lower() == 'rul' else NB_PATH\n",
    "DATA_DIR = ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "CONFIGS_DIR = ROOT / 'configs'\n",
    "\n",
    "for d in [DATA_DIR, RAW_DIR, PROCESSED_DIR, CONFIGS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Import helper from overview if needed\n",
    "print(\"ROOT:\", ROOT)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load configs\n",
    "def load_yaml(p): \n",
    "    import yaml, io\n",
    "    with open(p, 'r', encoding='utf-8') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "base_cfg   = load_yaml(CONFIGS_DIR / 'base.yaml')\n",
    "task_cfg   = load_yaml(CONFIGS_DIR / 'task_rul.yaml')\n",
    "line10_cfg = load_yaml(CONFIGS_DIR / 'line10.yaml') if (CONFIGS_DIR / 'line10.yaml').exists() else None\n",
    "line20_cfg = load_yaml(CONFIGS_DIR / 'line20.yaml') if (CONFIGS_DIR / 'line20.yaml').exists() else None\n",
    "\n",
    "base_cfg, task_cfg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6702d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load raw data (CSV; adapt for SQL/Influx if needed)\n",
    "def load_raw_from_cfg(line_cfg: dict) -> pd.DataFrame:\n",
    "    if line_cfg is None:\n",
    "        return pd.DataFrame()\n",
    "    src = line_cfg.get('source', 'csv')\n",
    "    path = line_cfg.get('path')\n",
    "    if src == 'csv' and path:\n",
    "        p = Path(path)\n",
    "        if p.exists():\n",
    "            df = pd.read_csv(p)\n",
    "            df['__line'] = line_cfg.get('line')\n",
    "            return df\n",
    "    print(\"WARN: Could not load\", line_cfg)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "df10 = load_raw_from_cfg(line10_cfg)\n",
    "df20 = load_raw_from_cfg(line20_cfg)\n",
    "\n",
    "df = pd.concat([df10, df20], ignore_index=True) if not df10.empty or not df20.empty else df10.copy()\n",
    "print(\"Shapes -> L10:\", df10.shape, \"| L20:\", df20.shape, \"| Combined:\", df.shape)\n",
    "display(df.head(3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b325e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Standardize columns\n",
    "time_col = base_cfg.get('time_index', 'timestamp')\n",
    "id_col   = base_cfg.get('id_col', 'machine_id')\n",
    "\n",
    "# Try to infer timestamp if not present\n",
    "if time_col not in df.columns:\n",
    "    for cand in ['timestamp','time','datetime','DateTime','ts','date']:\n",
    "        if cand in df.columns:\n",
    "            time_col = cand\n",
    "            break\n",
    "\n",
    "# Try to infer id if not present\n",
    "if id_col not in df.columns:\n",
    "    for cand in ['machine_id','machine','asset_id','AssetID','line_id']:\n",
    "        if cand in df.columns:\n",
    "            id_col = cand\n",
    "            break\n",
    "\n",
    "print(\"Using time_col:\", time_col, \"| id_col:\", id_col)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Parse timestamp & sort\n",
    "df[time_col] = pd.to_datetime(df[time_col], errors='coerce', utc=True)\n",
    "df = df.dropna(subset=[time_col]).sort_values([id_col, time_col]).reset_index(drop=True)\n",
    "\n",
    "# Basic EDA: missingness\n",
    "missing = df.isna().mean().sort_values(ascending=False).head(20)\n",
    "print(\"Top missing columns:\")\n",
    "display(missing.to_frame('missing_ratio'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59809502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Resample/regularize per machine (optional)\n",
    "# If you have sub-minute readings, configure base_cfg['freq'] accordingly.\n",
    "freq = base_cfg.get('freq', None)\n",
    "if freq:\n",
    "    # numeric-only resample (mean); keep id, line, and other constant columns forward-filled\n",
    "    non_num = df.select_dtypes(exclude='number').columns.tolist()\n",
    "    non_num = list(dict.fromkeys([c for c in non_num if c not in [time_col]] + [id_col, '__line']))\n",
    "    num = df.select_dtypes(include='number').columns.tolist()\n",
    "    num = [c for c in num if c not in [id_col] and c != '__line']\n",
    "\n",
    "    out_frames = []\n",
    "    for gid, g in df.groupby(id_col):\n",
    "        g = g.set_index(time_col).sort_index()\n",
    "        g_num = g[num].resample(freq).mean()\n",
    "        g_non = g[non_num].resample(freq).ffill().bfill()\n",
    "        g_ = pd.concat([g_non, g_num], axis=1)\n",
    "        g_[id_col] = gid\n",
    "        out_frames.append(g_.reset_index())\n",
    "\n",
    "    df = pd.concat(out_frames, ignore_index=True).sort_values([id_col, time_col]).reset_index(drop=True)\n",
    "\n",
    "print(\"After optional resample:\", df.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc39a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Build RUL Labels\n",
    "# Strategy:\n",
    "# 1) If a binary failure column exists (e.g., 'failure'==1), compute RUL as time to next failure (minutes).\n",
    "# 2) Otherwise, allow threshold-based failure criteria (placeholder).\n",
    "# 3) Records after the last failure per machine will have NaN RUL by default (can be trimmed or imputed).\n",
    "\n",
    "possible_failure_cols = [c for c in df.columns if c.lower() in {'failure','fail','is_failure','breakdown','fault'}]\n",
    "failure_col = possible_failure_cols[0] if possible_failure_cols else None\n",
    "\n",
    "def minutes_until_next_failure(g: pd.DataFrame, time_col: str, failure_col: str):\n",
    "    times = g[time_col].view('int64') // 10**9  # seconds\n",
    "    nxt_fail_time = np.full(len(g), np.nan)\n",
    "    next_time = None\n",
    "    for i in range(len(g)-1, -1, -1):\n",
    "        if failure_col and g.iloc[i][failure_col] in [1, True, '1']:\n",
    "            next_time = times.iloc[i]\n",
    "        if next_time is not None:\n",
    "            nxt_fail_time[i] = (next_time - times.iloc[i]) / 60.0\n",
    "    return nxt_fail_time\n",
    "\n",
    "if failure_col:\n",
    "    df['RUL'] = (\n",
    "        df.groupby(id_col, group_keys=False)\n",
    "          .apply(lambda g: pd.Series(minutes_until_next_failure(g, time_col, failure_col), index=g.index))\n",
    "          .astype(float)\n",
    "    )\n",
    "else:\n",
    "    # Placeholder: no explicit failure flag. You can implement your own logic here.\n",
    "    # Example: mark failures when some sensor exceeds a threshold, then compute RUL similarly.\n",
    "    df['RUL'] = np.nan\n",
    "    print(\"No failure column found. `RUL` set to NaN; please implement your criteria.\")\n",
    "\n",
    "display(df[[id_col, time_col, 'RUL']].head(10))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7dcefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Clean tails: optionally drop rows with NaN RUL (after last failure per machine)\n",
    "drop_na_rul = True\n",
    "if drop_na_rul:\n",
    "    before = len(df)\n",
    "    df = df[~df['RUL'].isna()].copy()\n",
    "    after = len(df)\n",
    "    print(f\"Dropped {before-after} rows with NaN RUL (post-last-failure tail).\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ac792",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save snapshot\n",
    "out_path = PROCESSED_DIR / 'rul_labeled.parquet'\n",
    "df.to_parquet(out_path, index=False)\n",
    "print(\"Saved:\", out_path, \"| shape:\", df.shape)\n",
    "\n",
    "# quick sanity: per-machine RUL distribution\n",
    "summary = df.groupby(id_col)['RUL'].agg(['count','min','max','median']).reset_index()\n",
    "display(summary.head(10))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
