{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f42621",
   "metadata": {},
   "source": [
    "# 02 — Feature Engineering (CoF, Streaming)\n",
    "\n",
    "This notebook streams per-machine to keep memory low.\n",
    "- Input: `data/processed/cof_labeled.parquet`\n",
    "- Shards: `data/processed/shards_CoF`\n",
    "- Target: `CoF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c75dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: d:\\Richard Files\\WORK\\pdm-project\n",
      "PROCESSED_DIR: d:\\Richard Files\\WORK\\pdm-project\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import joblib\n",
    "import shutil\n",
    "\n",
    "NB_PATH = Path.cwd()\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    cur = start\n",
    "    for _ in range(6):  # look up to 6 levels up\n",
    "        if (cur / \"requirements.txt\").exists() or (cur / \"configs\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    # Fallback: assume <repo>/notebooks/<task> structure\n",
    "    try:\n",
    "        i = [p.name.lower() for p in start.parents].index(\"notebooks\")\n",
    "        return start.parents[i+1]\n",
    "    except ValueError:\n",
    "        return start  # last resort\n",
    "\n",
    "ROOT = find_project_root(NB_PATH)\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n",
    "\n",
    "# --- If a mislabeled file exists under notebooks/<task>/data/processed, relocate it\n",
    "wrong_proc = NB_PATH / \"data\" / \"processed\"\n",
    "if wrong_proc.exists():\n",
    "    for name in [\"cof_labeled.parquet\", \"cof_labeled.csv\", \"rul_labeled.parquet\", \"rul_labeled.csv\"]:\n",
    "        src = wrong_proc / name\n",
    "        if src.exists():\n",
    "            dst = PROCESSED_DIR / name\n",
    "            try:\n",
    "                shutil.move(str(src), str(dst))\n",
    "                print(f\"Moved misplaced file → {dst}\")\n",
    "            except Exception as e:\n",
    "                print(\"Relocation warning:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74b977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using labeled file: d:\\Richard Files\\WORK\\pdm-project\\data\\processed\\cof_labeled.parquet\n"
     ]
    }
   ],
   "source": [
    "# --- Config\n",
    "id_col   = \"Mesin\"\n",
    "time_col = \"Timestamp\"\n",
    "target   = \"CoF\"\n",
    "\n",
    "LABELED   = PROCESSED_DIR / \"cof_labeled.parquet\"\n",
    "CSV_FALLB = PROCESSED_DIR / \"cof_labeled.csv\"\n",
    "\n",
    "SHARDS_DIR = PROCESSED_DIR / \"shards_CoF\"\n",
    "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Using labeled file:\", LABELED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd31fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (699840, 222) | From: d:\\Richard Files\\WORK\\pdm-project\\data\\processed\\cof_labeled.parquet\n"
     ]
    }
   ],
   "source": [
    "# --- Load labeled dataset (robust + fallback)\n",
    "if not LABELED.exists() and not CSV_FALLB.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing labeled file:\\n  {LABELED}\\n(or CSV fallback)\\n\"\n",
    "        \"→ Run notebooks/CoF/01_eda_data_prep_CoF.ipynb to generate it.\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    if LABELED.exists():\n",
    "        df = pd.read_parquet(LABELED)\n",
    "        src = LABELED\n",
    "    else:\n",
    "        df = pd.read_csv(CSV_FALLB, parse_dates=[time_col])\n",
    "        src = CSV_FALLB\n",
    "except Exception as e:\n",
    "    # Parquet engine missing? Fall back to CSV if available\n",
    "    if \"pyarrow\" in str(e).lower() and CSV_FALLB.exists():\n",
    "        df = pd.read_csv(CSV_FALLB, parse_dates=[time_col])\n",
    "        src = CSV_FALLB\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "df = df.sort_values([id_col, time_col]).reset_index(drop=True)\n",
    "print(\"Loaded:\", df.shape, \"| From:\", src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4f0d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric feature columns detected: 217\n",
      "['Start_Time', 'End_Time', 'Level_1', 'Level_2', 'Detail', 'Energy_Sectors.Extruder_10_Target', 'EXT_10.Extruder_Load', 'EXT_10.Machine_Hour', 'EXT_10.Machine_Run', 'EXT_10.Melt_Press']\n",
      "timestamp                            datetime64[ns]\n",
      "Start_Time                                  float64\n",
      "End_Time                                    float64\n",
      "Level_1                                     float64\n",
      "Level_2                                     float64\n",
      "Detail                                      float64\n",
      "machine_id                                    int64\n",
      "__line                                        int64\n",
      "Energy_Sectors.Extruder_10_Target           float64\n",
      "EXT_10.Extruder_Load                        float64\n",
      "EXT_10.Machine_Hour                         float64\n",
      "EXT_10.Machine_Run                          float64\n",
      "EXT_10.Melt_Press                           float64\n",
      "EXT_10.Melt_Temp_1                          float64\n",
      "EXT_10.Motor_Extruder_Run                   float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# --- Normalize schema and ensure numeric dtypes\n",
    "df = df.rename(columns={\"Mesin\": \"machine_id\", \"Timestamp\": \"timestamp\"})  # unify naming\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Identify potential numeric sensor columns\n",
    "non_feature = {\"machine_id\", \"timestamp\", \"Breakdown\", \"CoF\", \"__line\"}\n",
    "num_cols = [c for c in df.columns if c not in non_feature]\n",
    "\n",
    "# Convert any object-like columns to numeric where possible\n",
    "for c in num_cols:\n",
    "    if df[c].dtype == \"object\":\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(\"Numeric feature columns detected:\", len(num_cols))\n",
    "print(num_cols[:10])\n",
    "print(df.dtypes.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "566adb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric sensors: 218\n"
     ]
    }
   ],
   "source": [
    "# --- Column selection & downcast\n",
    "exclude = {id_col, time_col, '__line', target}\n",
    "exclude |= {c for c in df.columns if c.lower() in {'breakdown','failure','fail','is_failure'}}\n",
    "num_cols = [c for c in df.select_dtypes(include='number').columns if c not in exclude]\n",
    "print(\"Numeric sensors:\", len(num_cols))\n",
    "\n",
    "def downcast_numeric(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in g.select_dtypes(include='float').columns:\n",
    "        g[c] = pd.to_numeric(g[c], downcast='float')\n",
    "    for c in g.select_dtypes(include='integer').columns:\n",
    "        g[c] = pd.to_numeric(g[c], downcast='integer')\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5512326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Lightweight feature engineering\n",
    "def add_lags(g, cols, lags=(1,)):\n",
    "    for L in lags:\n",
    "        for c in cols:\n",
    "            g[f'{c}_lag{L}'] = g[c].shift(L)\n",
    "    return g\n",
    "\n",
    "def add_roll_stats(g, cols, windows=(3,5)):\n",
    "    for w in windows:\n",
    "        roll = g[cols].rolling(w, min_periods=w)\n",
    "        g[[f'{c}_roll{w}_mean' for c in cols]] = roll.mean().values\n",
    "        g[[f'{c}_roll{w}_std'  for c in cols]] = roll.std().values\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f70352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base numeric features: 217\n",
      "✓ Shard part_machine_id_10.parquet: 349917 rows, 434 features\n",
      "✓ Shard part_machine_id_20.parquet: 349917 rows, 434 features\n",
      "Shards written: 2 → d:\\Richard Files\\WORK\\pdm-project\\data\\processed\\shards_CoF\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "LAGS = (1,)          # keep light first; add (2,) later if OK\n",
    "WINS = (3,)          # start with (3,) only; add (5,) later if OK\n",
    "BATCH = 32           # number of columns per batch; tune to your RAM\n",
    "\n",
    "id_col, time_col, target = \"machine_id\", \"timestamp\", \"CoF\"\n",
    "shard_paths = []\n",
    "\n",
    "# base feature candidates = numeric sensors only\n",
    "non_feature = {id_col, time_col, target, \"Breakdown\", \"__line\"}\n",
    "feats_base = [c for c in df.select_dtypes(include=\"number\").columns if c not in non_feature]\n",
    "print(f\"Base numeric features: {len(feats_base)}\")\n",
    "\n",
    "def chunked(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "for gid, g0 in df.groupby(id_col, sort=True):\n",
    "    # minimal frame we’ll keep growing\n",
    "    g = g0[[id_col, time_col, target]].copy()\n",
    "    g[time_col] = pd.to_datetime(g[time_col], errors=\"coerce\")\n",
    "\n",
    "    # === per-batch feature engineering to keep memory low ===\n",
    "    for cols in chunked(feats_base, BATCH):\n",
    "        block = g0[cols].astype(\"float32\").copy()\n",
    "\n",
    "        # lags\n",
    "        for L in LAGS:\n",
    "            lagged = block.shift(L)\n",
    "            lagged.columns = [f\"{c}_lag{L}\" for c in cols]\n",
    "            g = pd.concat([g, lagged], axis=1)\n",
    "            del lagged\n",
    "            gc.collect()\n",
    "\n",
    "        # rolling MEAN only (add STD later if RAM allows)\n",
    "        for w in WINS:\n",
    "            roll_mean = block.rolling(w, min_periods=w).mean()\n",
    "            roll_mean.columns = [f\"{c}_r{w}_mean\" for c in cols]\n",
    "            g = pd.concat([g, roll_mean], axis=1)\n",
    "            del roll_mean\n",
    "            gc.collect()\n",
    "\n",
    "        del block\n",
    "        gc.collect()\n",
    "\n",
    "    # warm-up trim (avoid first rows that are NaN due to lags/rolling)\n",
    "    warmup = max(LAGS or (0,)) + max(WINS or (1,)) - 1\n",
    "    if warmup > 0 and len(g) > warmup:\n",
    "        g = g.iloc[warmup:].copy()\n",
    "\n",
    "    # gentle fill so we don’t drop everything\n",
    "    feat_cols_all = [c for c in g.columns if c not in {id_col, time_col, target}]\n",
    "    g[feat_cols_all] = g[feat_cols_all].ffill().bfill()\n",
    "\n",
    "    # drop rows only if ALL engineered features are NaN (rare now)\n",
    "    g = g.dropna(subset=feat_cols_all, how=\"all\")\n",
    "\n",
    "    if len(g) == 0:\n",
    "        print(f\"⚠️ Empty after warm-up/ffill: machine={gid} — skipping\")\n",
    "        continue\n",
    "\n",
    "    out_path = SHARDS_DIR / f\"part_{id_col}_{gid}.parquet\"\n",
    "    g.to_parquet(out_path, index=False)\n",
    "    shard_paths.append(out_path)\n",
    "    print(f\"✓ Shard {out_path.name}: {len(g)} rows, {len(feat_cols_all)} features\")\n",
    "\n",
    "print(f\"Shards written: {len(shard_paths)} → {SHARDS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "718a2b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard files: 4\n",
      "part_machine_id_10.parquet | rows: 349917\n",
      "part_machine_id_20.parquet | rows: 349917\n",
      "part_Mesin_10.parquet | rows: 0\n",
      "part_Mesin_20.parquet | rows: 0\n"
     ]
    }
   ],
   "source": [
    "paths = sorted(SHARDS_DIR.glob(\"part_*.parquet\"))\n",
    "print(\"Shard files:\", len(paths))\n",
    "for p in paths[:5]:\n",
    "    g = pd.read_parquet(p)\n",
    "    print(p.name, \"| rows:\", len(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9587e89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scaler fitted on 349917 rows\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np, pandas as pd, joblib\n",
    "\n",
    "scaler = StandardScaler()\n",
    "sample_rows = 100_000\n",
    "X_buf = None\n",
    "rows_used = 0\n",
    "\n",
    "for p in shard_paths:\n",
    "    g = pd.read_parquet(p)\n",
    "    feats = [c for c in g.columns if c not in [id_col, time_col, target]]\n",
    "    if len(g) == 0 or not feats:\n",
    "        print(\"Skip (empty/no feats):\", p.name)\n",
    "        continue\n",
    "    # at least some data per row\n",
    "    mask = g[feats].notna().any(axis=1)\n",
    "    if not mask.any():\n",
    "        print(\"Skip (all-NaN rows):\", p.name)\n",
    "        continue\n",
    "    X_chunk = g.loc[mask, feats].fillna(0.0).values\n",
    "    X_buf = X_chunk if X_buf is None else np.vstack([X_buf, X_chunk])\n",
    "    rows_used += X_chunk.shape[0]\n",
    "    if rows_used >= sample_rows:\n",
    "        break\n",
    "\n",
    "if X_buf is None or X_buf.shape[0] == 0:\n",
    "    raise RuntimeError(\"No valid data found to fit StandardScaler — inspect shards again.\")\n",
    "\n",
    "joblib.dump(scaler.fit(X_buf), PROCESSED_DIR / \"cof_scaler.joblib\")\n",
    "print(\"✅ Scaler fitted on\", X_buf.shape[0], \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4eb911d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Richard Files\\WORK\\pdm-project\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: -1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m val_idx = np.arange(test_idx[\u001b[32m0\u001b[39m], mid+\u001b[32m1\u001b[39m)\n\u001b[32m     13\u001b[39m test_idx2 = np.arange(mid+\u001b[32m1\u001b[39m, test_idx[-\u001b[32m1\u001b[39m]+\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m cut_train_t = \u001b[43mmeta\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m[time_col]\n\u001b[32m     16\u001b[39m cut_val_t   = meta.iloc[val_idx][-\u001b[32m1\u001b[39m][time_col]\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mCutoffs:\u001b[39m\u001b[33m'\u001b[39m, cut_train_t, \u001b[33m'\u001b[39m\u001b[33m|\u001b[39m\u001b[33m'\u001b[39m, cut_val_t)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Richard Files\\WORK\\pdm-project\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Richard Files\\WORK\\pdm-project\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: -1"
     ]
    }
   ],
   "source": [
    "# --- Build global time ordering to compute cutoffs for splits\n",
    "meta_parts = []\n",
    "for p in shard_paths:\n",
    "    g = pd.read_parquet(p, columns=[time_col])\n",
    "    meta_parts.append(g.assign(path=str(p)))\n",
    "meta = pd.concat(meta_parts, ignore_index=True).sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "splits = list(tscv.split(meta))\n",
    "train_idx, test_idx = splits[-1]\n",
    "mid = (test_idx[0] + test_idx[-1]) // 2\n",
    "val_idx = np.arange(test_idx[0], mid+1)\n",
    "test_idx2 = np.arange(mid+1, test_idx[-1]+1)\n",
    "\n",
    "cut_train_t = meta.iloc[train_idx][-1][time_col]\n",
    "cut_val_t   = meta.iloc[val_idx][-1][time_col]\n",
    "print('Cutoffs:', cut_train_t, '|', cut_val_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper to append parquet\n",
    "from pathlib import Path\n",
    "def append_parquet(df, path: Path):\n",
    "    if path.exists():\n",
    "        old = pd.read_parquet(path)\n",
    "        pd.concat([old, df], ignore_index=True).to_parquet(path, index=False)\n",
    "    else:\n",
    "        df.to_parquet(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stream shards → scale & route to split\n",
    "Xtr_path = PROCESSED_DIR / 'CoF_X_train.parquet'\n",
    "Xva_path = PROCESSED_DIR / 'CoF_X_val.parquet'\n",
    "Xte_path = PROCESSED_DIR / 'CoF_X_test.parquet'\n",
    "ytr_path = PROCESSED_DIR / 'CoF_y_train.parquet'\n",
    "yva_path = PROCESSED_DIR / 'CoF_y_val.parquet'\n",
    "yte_path = PROCESSED_DIR / 'CoF_y_test.parquet'\n",
    "\n",
    "for p in [Xtr_path, Xva_path, Xte_path, ytr_path, yva_path, yte_path]:\n",
    "    if p.exists():\n",
    "        p.unlink()\n",
    "\n",
    "for p in shard_paths:\n",
    "    g = pd.read_parquet(p)\n",
    "    feats = [c for c in g.columns if c not in [id_col, time_col, target]]\n",
    "    keep  = [id_col, time_col]\n",
    "    X_scaled = pd.DataFrame(scaler.transform(g[feats].values), columns=feats)\n",
    "    X_scaled = pd.concat([g[keep].reset_index(drop=True), X_scaled.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    mask_tr = g[time_col] <= cut_train_t\n",
    "    mask_va = (g[time_col] > cut_train_t) & (g[time_col] <= cut_val_t)\n",
    "    mask_te = g[time_col] > cut_val_t\n",
    "\n",
    "    append_parquet(X_scaled.loc[mask_tr], Xtr_path)\n",
    "    append_parquet(X_scaled.loc[mask_va], Xva_path)\n",
    "    append_parquet(X_scaled.loc[mask_te], Xte_path)\n",
    "\n",
    "    append_parquet(pd.DataFrame({target: g.loc[mask_tr, target].values}), ytr_path)\n",
    "    append_parquet(pd.DataFrame({target: g.loc[mask_va, target].values}), yva_path)\n",
    "    append_parquet(pd.DataFrame({target: g.loc[mask_te, target].values}), yte_path)\n",
    "\n",
    "print('Saved datasets:')\n",
    "print('  ', Xtr_path.name, Xva_path.name, Xte_path.name)\n",
    "print('  ', ytr_path.name, yva_path.name, yte_path.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
