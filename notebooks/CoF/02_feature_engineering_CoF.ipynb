{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81fbb306",
   "metadata": {},
   "source": [
    "\n",
    "# 02 â€” Feature Engineering (CoF)\n",
    "\n",
    "- Load `data/processed/cof_labeled.parquet`\n",
    "- Create time-series features (lags, rolling stats, diffs)\n",
    "- Time-series split (train/val/test) and scaling\n",
    "- Save `CoF_X/*.parquet`, `CoF_y/*.parquet`, and a fitted scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52fc850",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "NB_PATH = Path.cwd()\n",
    "ROOT = NB_PATH.parents[1] if NB_PATH.name.lower() == 'cof' else NB_PATH\n",
    "DATA_DIR = ROOT / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "id_col   = 'Mesin'       # keep in sync with configs\n",
    "time_col = 'Timestamp'\n",
    "target   = 'CoF'\n",
    "breakdown_col = 'Breakdown'  # if present; will be excluded from features\n",
    "\n",
    "# --- Load & sort\n",
    "df = pd.read_parquet(PROCESSED_DIR / 'cof_labeled.parquet')\n",
    "df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
    "df = df.dropna(subset=[time_col]).sort_values([id_col, time_col]).reset_index(drop=True)\n",
    "\n",
    "print(\"Loaded:\", df.shape, \"| positives:\", int(df[target].sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0309adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Select numeric sensor columns (exclude id/time/line/targets/flags)\n",
    "exclude = {id_col, time_col, target, '__line'}\n",
    "exclude |= {c for c in df.columns if c.lower() in {'breakdown','failure','fail','is_failure','rul'}}\n",
    "num_cols = [c for c in df.select_dtypes(include='number').columns if c not in exclude]\n",
    "print(\"Numeric sensor columns:\", len(num_cols))\n",
    "num_cols[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Feature engineering helpers\n",
    "def add_lags(g, cols, lags=(1,2,3)):\n",
    "    for L in lags:\n",
    "        for c in cols:\n",
    "            g[f'{c}_lag{L}'] = g[c].shift(L)\n",
    "    return g\n",
    "\n",
    "def add_roll_stats(g, cols, windows=(3,5,15)):\n",
    "    for w in windows:\n",
    "        for c in cols:\n",
    "            g[f'{c}_roll{w}_mean'] = g[c].rolling(w).mean()\n",
    "            g[f'{c}_roll{w}_std']  = g[c].rolling(w).std()\n",
    "            g[f'{c}_roll{w}_min']  = g[c].rolling(w).min()\n",
    "            g[f'{c}_roll{w}_max']  = g[c].rolling(w).max()\n",
    "    return g\n",
    "\n",
    "def add_diffs(g, cols):\n",
    "    for c in cols:\n",
    "        g[f'{c}_diff1'] = g[c].diff(1)\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3240bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Apply per machine to avoid leakage\n",
    "fe = []\n",
    "for gid, g in df.groupby(id_col):\n",
    "    g = g.copy()\n",
    "    g = add_lags(g, num_cols, lags=(1,2,3))\n",
    "    g = add_roll_stats(g, num_cols, windows=(3,5,15))\n",
    "    g = add_diffs(g, num_cols)\n",
    "    fe.append(g)\n",
    "\n",
    "df_fe = pd.concat(fe, ignore_index=True).dropna().reset_index(drop=True)\n",
    "print(\"After FE:\", df_fe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e7401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Split by time using TimeSeriesSplit\n",
    "X = df_fe.drop(columns=[target])\n",
    "y = df_fe[target].values\n",
    "\n",
    "keep = [id_col, time_col, '__line'] if '__line' in X.columns else [id_col, time_col]\n",
    "X_num = X.drop(columns=keep, errors='ignore')\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_num_scaled = scaler.fit_transform(X_num.values)\n",
    "X_scaled = pd.DataFrame(X_num_scaled, columns=X_num.columns, index=X_num.index)\n",
    "X_scaled = pd.concat([X[keep], X_scaled], axis=1)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(scaler, PROCESSED_DIR / 'cof_scaler.joblib')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "splits = list(tscv.split(X_scaled))\n",
    "(train_idx, test_idx) = splits[-1]\n",
    "mid = (test_idx[0] + test_idx[-1]) // 2\n",
    "val_idx = np.arange(test_idx[0], mid+1)\n",
    "test_idx2 = np.arange(mid+1, test_idx[-1]+1)\n",
    "\n",
    "print(\"Index sizes -> train:\", len(train_idx), \"val:\", len(val_idx), \"test:\", len(test_idx2))\n",
    "\n",
    "# Save parquet datasets\n",
    "X_scaled.iloc[train_idx].to_parquet(PROCESSED_DIR / 'CoF_X_train.parquet', index=False)\n",
    "X_scaled.iloc[val_idx].to_parquet(PROCESSED_DIR / 'CoF_X_val.parquet', index=False)\n",
    "X_scaled.iloc[test_idx2].to_parquet(PROCESSED_DIR / 'CoF_X_test.parquet', index=False)\n",
    "\n",
    "pd.DataFrame({target: y[train_idx]}).to_parquet(PROCESSED_DIR / 'CoF_y_train.parquet', index=False)\n",
    "pd.DataFrame({target: y[val_idx]}).to_parquet(PROCESSED_DIR / 'CoF_y_val.parquet', index=False)\n",
    "pd.DataFrame({target: y[test_idx2]}).to_parquet(PROCESSED_DIR / 'CoF_y_test.parquet', index=False)\n",
    "\n",
    "print(\"Saved processed train/val/test and scaler.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
