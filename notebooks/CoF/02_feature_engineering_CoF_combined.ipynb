{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad3796b",
   "metadata": {},
   "source": [
    "\n",
    "# 02 — Feature Engineering (CoF, Combined Line 10 & Line 20)\n",
    "\n",
    "This notebook builds **Chance of Failure (CoF)** features from **both Line 10 and Line 20**, even if they have **different sensor columns**:\n",
    "\n",
    "- Robust project-root detection\n",
    "- Per-line, **batched** feature engineering with `float32` (memory-safe)\n",
    "- **Union schema** alignment so both lines share the same feature columns\n",
    "- **Time split per line (80/10/10)**, then concatenated into combined train/val/test\n",
    "- **No scaling** required (tree models handle raw/NaN well). You can enable scaling later if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706d11e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: d:\\Richard Files\\WORK\\pdm-project\n",
      "PROCESSED_DIR: d:\\Richard Files\\WORK\\pdm-project\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import joblib\n",
    "import shutil\n",
    "\n",
    "NB_PATH = Path.cwd()\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    cur = start\n",
    "    for _ in range(6):  # look up to 6 levels up\n",
    "        if (cur / \"requirements.txt\").exists() or (cur / \"configs\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    # Fallback: assume <repo>/notebooks/<task> structure\n",
    "    try:\n",
    "        i = [p.name.lower() for p in start.parents].index(\"notebooks\")\n",
    "        return start.parents[i+1]\n",
    "    except ValueError:\n",
    "        return start  # last resort\n",
    "\n",
    "ROOT = find_project_root(NB_PATH)\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n",
    "\n",
    "# --- If a mislabeled file exists under notebooks/<task>/data/processed, relocate it\n",
    "wrong_proc = NB_PATH / \"data\" / \"processed\"\n",
    "if wrong_proc.exists():\n",
    "    for name in [\"cof_labeled.parquet\", \"cof_labeled.csv\", \"rul_labeled.parquet\", \"rul_labeled.csv\"]:\n",
    "        src = wrong_proc / name\n",
    "        if src.exists():\n",
    "            dst = PROCESSED_DIR / name\n",
    "            try:\n",
    "                shutil.move(str(src), str(dst))\n",
    "                print(f\"Moved misplaced file → {dst}\")\n",
    "            except Exception as e:\n",
    "                print(\"Relocation warning:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7888212d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled path: d:\\Richard Files\\WORK\\pdm-project\\data\\processed\\cof_labeled.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Config\n",
    "id_col   = \"machine_id\"\n",
    "time_col = \"timestamp\"\n",
    "target   = \"CoF\"      # binary label (0/1)\n",
    "\n",
    "# Expect a labeled merged file from 01 (may or may not include __line)\n",
    "LABELED   = PROCESSED_DIR / \"cof_labeled.parquet\"\n",
    "CSV_FALLB = PROCESSED_DIR / \"cof_labeled.csv\"\n",
    "\n",
    "# Per-line shard output\n",
    "SHARDS_L10 = PROCESSED_DIR / \"shards_CoF_L10\"\n",
    "SHARDS_L20 = PROCESSED_DIR / \"shards_CoF_L20\"\n",
    "for p in [SHARDS_L10, SHARDS_L20]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Labeled path:\", LABELED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8ecd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (699840, 222) | From: d:\\Richard Files\\WORK\\pdm-project\\data\\processed\\cof_labeled.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Load labeled dataset (robust + fallback)\n",
    "if not LABELED.exists() and not CSV_FALLB.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing labeled file:\\n  {LABELED}\\n(or CSV fallback)\\n\"\n",
    "        \"→ Run notebooks/CoF/01_eda_data_prep_CoF.ipynb to generate it.\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    if LABELED.exists():\n",
    "        df = pd.read_parquet(LABELED)\n",
    "        src = LABELED\n",
    "    else:\n",
    "        df = pd.read_csv(CSV_FALLB)\n",
    "        src = CSV_FALLB\n",
    "except Exception as e:\n",
    "    # Parquet engine missing? Fall back to CSV if available\n",
    "    if \"pyarrow\" in str(e).lower() and CSV_FALLB.exists():\n",
    "        df = pd.read_csv(CSV_FALLB)\n",
    "        src = CSV_FALLB\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(\"Loaded:\", df.shape, \"| From:\", src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d475fd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 222\n",
      "Lines present (unique __line): [10 20]\n",
      "   machine_id           timestamp  __line  CoF\n",
      "0          10 2025-01-01 00:00:00      10    0\n",
      "1          10 2025-01-01 00:01:00      10    0\n",
      "2          10 2025-01-01 00:02:00      10    0\n",
      "3          10 2025-01-01 00:03:00      10    0\n",
      "4          10 2025-01-01 00:04:00      10    0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Normalize schema: Mesin→machine_id, Timestamp→timestamp; ensure datetime; ensure __line\n",
    "df = df.rename(columns={\"Mesin\":\"machine_id\", \"Timestamp\":\"timestamp\"})\n",
    "df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
    "\n",
    "# Ensure __line present; try to infer if missing\n",
    "if \"__line\" not in df.columns:\n",
    "    if \"Line\" in df.columns:\n",
    "        df[\"__line\"] = df[\"Line\"]\n",
    "    else:\n",
    "        df[\"__line\"] = np.nan  # if unknown, still works as single group\n",
    "\n",
    "# Ensure target present\n",
    "if target not in df.columns:\n",
    "    if \"Breakdown\" in df.columns:\n",
    "        df[target] = (df[\"Breakdown\"].astype(float) > 0).astype(int)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Target column '{target}' not found and cannot infer from 'Breakdown'.\")\n",
    "\n",
    "# Convert object-like sensor cols to numeric where possible\n",
    "non_feature = {id_col, time_col, \"__line\", target, \"Breakdown\", \"RUL\"}\n",
    "for c in df.columns:\n",
    "    if c not in non_feature and df[c].dtype == \"object\":\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values([id_col, time_col]).reset_index(drop=True)\n",
    "\n",
    "print(\"Columns:\", len(df.columns))\n",
    "print(\"Lines present (unique __line):\", df[\"__line\"].dropna().unique()[:10])\n",
    "print(df[[id_col, time_col, \"__line\", target]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d84aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Memory-safe feature engineering helpers\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "LAGS  = (1,)     # keep light; expand later\n",
    "WINS  = (3,)     # keep light; expand later\n",
    "BATCH = 32       # smaller batch = lower peak RAM\n",
    "COVERAGE_MIN = 0.85   # require ≥85% non-null per raw sensor\n",
    "MAX_FEATS     = 800   # hard cap to avoid explosion\n",
    "\n",
    "def _raw_sensor_list(df_line, id_col=\"machine_id\", time_col=\"timestamp\", target=\"CoF\"):\n",
    "    \"\"\"Return ONLY raw numeric sensors (no engineered suffixes).\"\"\"\n",
    "    skip = {id_col, time_col, \"__line\", target, \"Breakdown\", \"RUL\"}\n",
    "    cand = [c for c in df_line.select_dtypes(include=\"number\").columns if c not in skip]\n",
    "    # exclude previously engineered columns if cell is re-run\n",
    "    raw = [c for c in cand if (\"_lag\" not in c and \"_r\" not in c)]\n",
    "    return raw\n",
    "\n",
    "def _prune_by_coverage(df_line, feats, threshold=COVERAGE_MIN, limit=MAX_FEATS):\n",
    "    if not feats:\n",
    "        return []\n",
    "    cov = df_line[feats].notna().mean().sort_values(ascending=False)\n",
    "    keep = cov[cov >= threshold].index.tolist()\n",
    "    if not keep:  # if too strict, soften to top-N by coverage\n",
    "        keep = cov.index.tolist()\n",
    "    if limit:\n",
    "        keep = keep[:limit]\n",
    "    return keep\n",
    "\n",
    "def _chunked(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "def engineer_per_line_to_shards(df_line: pd.DataFrame, out_dir: Path):\n",
    "    id_col, time_col, target = \"machine_id\", \"timestamp\", \"CoF\"\n",
    "    shard_paths = []\n",
    "\n",
    "    # 1) pick RAW sensors only\n",
    "    feats_base_raw = _raw_sensor_list(df_line, id_col, time_col, target)\n",
    "    feats_base = _prune_by_coverage(df_line, feats_base_raw, COVERAGE_MIN, MAX_FEATS)\n",
    "    print(f\"Raw sensors available: {len(feats_base_raw)} | after coverage/cap: {len(feats_base)}\")\n",
    "    if len(feats_base_raw) != len(feats_base):\n",
    "        print(\"ℹ️ Pruned low-coverage or excess sensors to keep memory stable.\")\n",
    "\n",
    "    # 2) group per machine\n",
    "    for gid, g0 in df_line.groupby(id_col, sort=True):\n",
    "        g = g0[[id_col, time_col, \"__line\", target]].copy()\n",
    "        g[time_col] = pd.to_datetime(g[time_col], errors=\"coerce\")\n",
    "\n",
    "        # 3) engineer in batches with minimal concat\n",
    "        for cols in _chunked(feats_base, BATCH):\n",
    "            block = g0[cols].astype(\"float32\").copy()\n",
    "            to_concat = [g]  # always include current g (left side)\n",
    "\n",
    "            # lags\n",
    "            for L in LAGS:\n",
    "                lagged = block.shift(L)\n",
    "                lagged.columns = [f\"{c}_lag{L}\" for c in cols]\n",
    "                to_concat.append(lagged)\n",
    "\n",
    "            # rolling mean (add std later if RAM allows)\n",
    "            for w in WINS:\n",
    "                roll_mean = block.rolling(w, min_periods=w).mean()\n",
    "                roll_mean.columns = [f\"{c}_r{w}_mean\" for c in cols]\n",
    "                to_concat.append(roll_mean)\n",
    "\n",
    "            # single concat per batch (faster & less RAM than many concats)\n",
    "            g = pd.concat(to_concat, axis=1)\n",
    "            del block, to_concat\n",
    "            gc.collect()\n",
    "\n",
    "        # 4) warm-up trim instead of global dropna\n",
    "        warmup = max(LAGS or (0,)) + max(WINS or (1,)) - 1\n",
    "        if warmup > 0 and len(g) > warmup:\n",
    "            g = g.iloc[warmup:].copy()\n",
    "\n",
    "        # 5) gentle fill for features only (never touch target)\n",
    "        feat_cols_all = [c for c in g.columns if c not in {id_col, time_col, \"__line\", target}]\n",
    "        if feat_cols_all:\n",
    "            g[feat_cols_all] = g[feat_cols_all].ffill().bfill()\n",
    "            # drop rows only if ALL engineered features are NaN\n",
    "            g = g.dropna(subset=feat_cols_all, how=\"all\")\n",
    "\n",
    "        if len(g) == 0:\n",
    "            print(f\"⚠️ Empty after warm-up/ffill: machine={gid} — skipping\")\n",
    "            continue\n",
    "\n",
    "        out_path = out_dir / f\"part_{id_col}_{gid}.parquet\"\n",
    "        g.to_parquet(out_path, index=False)\n",
    "        shard_paths.append(out_path)\n",
    "        print(f\"✓ {out_path.name}: rows={len(g)}, feats={len(feat_cols_all)}\")\n",
    "\n",
    "        del g\n",
    "        gc.collect()\n",
    "\n",
    "    return shard_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03700ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw sensors available: 217 | after coverage/cap: 68\n",
      "ℹ️ Pruned low-coverage or excess sensors to keep memory stable.\n",
      "✓ part_machine_id_10.parquet: rows=349917, feats=136\n",
      "Raw sensors available: 217 | after coverage/cap: 74\n",
      "ℹ️ Pruned low-coverage or excess sensors to keep memory stable.\n",
      "✓ part_machine_id_20.parquet: rows=349917, feats=148\n",
      "Shards L10: 1\n",
      "Shards L20: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Engineer per line → shards\n",
    "has_lines = df[\"__line\"].notna().any()\n",
    "paths_L10, paths_L20 = [], []\n",
    "\n",
    "if has_lines:\n",
    "    if (df[\"__line\"] == 10).any():\n",
    "        paths_L10 = engineer_per_line_to_shards(df[df[\"__line\"] == 10], SHARDS_L10)\n",
    "    if (df[\"__line\"] == 20).any():\n",
    "        paths_L20 = engineer_per_line_to_shards(df[df[\"__line\"] == 20], SHARDS_L20)\n",
    "else:\n",
    "    df[\"__line\"] = 10\n",
    "    paths_L10 = engineer_per_line_to_shards(df, SHARDS_L10)\n",
    "\n",
    "print(\"Shards L10:\", len(paths_L10))\n",
    "print(\"Shards L20:\", len(paths_L20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18d45c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union features: 282\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Build union schema from a sample of shards\n",
    "ignore = {id_col, time_col, \"__line\", target}\n",
    "def collect_feats(paths, k=5):\n",
    "    feats = set()\n",
    "    for p in sorted(paths)[:k]:\n",
    "        g = pd.read_parquet(p)\n",
    "        feats |= set([c for c in g.columns if c not in ignore])\n",
    "    return feats\n",
    "\n",
    "feat10 = collect_feats(paths_L10) if paths_L10 else set()\n",
    "feat20 = collect_feats(paths_L20) if paths_L20 else set()\n",
    "ALL_FEATS = sorted(feat10 | feat20)\n",
    "print(\"Union features:\", len(ALL_FEATS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19d56b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Align shard to union schema\n",
    "def align_union(g: pd.DataFrame, all_feats, target: str) -> pd.DataFrame:\n",
    "    g = g.copy()\n",
    "    for c in all_feats:\n",
    "        if c not in g.columns:\n",
    "            g[c] = pd.NA\n",
    "    cols = [id_col, time_col, \"__line\"] + all_feats + [target]\n",
    "    return g[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6b8789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoffs L10: 2025-07-14 09:35:48 | 2025-08-07 16:47:24\n",
      "Cutoffs L20: 2025-07-14 09:35:48 | 2025-08-07 16:47:24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Compute time cutoffs per line (80/10/10)\n",
    "def cutoffs_for(paths):\n",
    "    if not paths: \n",
    "        return None, None\n",
    "    meta = []\n",
    "    for p in paths:\n",
    "        gg = pd.read_parquet(p, columns=[time_col])\n",
    "        gg[time_col] = pd.to_datetime(gg[time_col], errors=\"coerce\")\n",
    "        meta.append(gg)\n",
    "    meta = pd.concat(meta, ignore_index=True).dropna(subset=[time_col])\n",
    "    if meta.empty:\n",
    "        return None, None\n",
    "    q1 = meta[time_col].quantile(0.80)\n",
    "    q2 = meta[time_col].quantile(0.90)\n",
    "    return q1, q2\n",
    "\n",
    "cut10_tr, cut10_va = cutoffs_for(paths_L10)\n",
    "cut20_tr, cut20_va = cutoffs_for(paths_L20)\n",
    "print(\"Cutoffs L10:\", cut10_tr, \"|\", cut10_va)\n",
    "print(\"Cutoffs L20:\", cut20_tr, \"|\", cut20_va)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cd43ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Helper to append parquet\n",
    "from pathlib import Path\n",
    "def append_parquet(df, path: Path):\n",
    "    if path.exists():\n",
    "        old = pd.read_parquet(path)\n",
    "        pd.concat([old, df], ignore_index=True).to_parquet(path, index=False)\n",
    "    else:\n",
    "        df.to_parquet(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53ca6ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n",
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\1408578516.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  g[c] = pd.NA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-line splits written (if available).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Stream & split per line, write *_L10.parquet / *_L20.parquet\n",
    "Xtr_L10 = PROCESSED_DIR / \"CoF_X_train_L10.parquet\"\n",
    "Xva_L10 = PROCESSED_DIR / \"CoF_X_val_L10.parquet\"\n",
    "Xte_L10 = PROCESSED_DIR / \"CoF_X_test_L10.parquet\"\n",
    "ytr_L10 = PROCESSED_DIR / \"CoF_y_train_L10.parquet\"\n",
    "yva_L10 = PROCESSED_DIR / \"CoF_y_val_L10.parquet\"\n",
    "yte_L10 = PROCESSED_DIR / \"CoF_y_test_L10.parquet\"\n",
    "\n",
    "Xtr_L20 = PROCESSED_DIR / \"CoF_X_train_L20.parquet\"\n",
    "Xva_L20 = PROCESSED_DIR / \"CoF_X_val_L20.parquet\"\n",
    "Xte_L20 = PROCESSED_DIR / \"CoF_X_test_L20.parquet\"\n",
    "ytr_L20 = PROCESSED_DIR / \"CoF_y_train_L20.parquet\"\n",
    "yva_L20 = PROCESSED_DIR / \"CoF_y_val_L20.parquet\"\n",
    "yte_L20 = PROCESSED_DIR / \"CoF_y_test_L20.parquet\"\n",
    "\n",
    "for p in [Xtr_L10, Xva_L10, Xte_L10, ytr_L10, yva_L10, yte_L10,\n",
    "          Xtr_L20, Xva_L20, Xte_L20, ytr_L20, yva_L20, yte_L20]:\n",
    "    if p.exists():\n",
    "        p.unlink()\n",
    "\n",
    "def route_and_write(paths, cut_tr, cut_va, tag):\n",
    "    if not paths or cut_tr is None or cut_va is None:\n",
    "        print(f\"Skipping line {tag}: missing paths or cutoffs\")\n",
    "        return\n",
    "\n",
    "    for p in paths:\n",
    "        g = pd.read_parquet(p)\n",
    "        g = align_union(g, ALL_FEATS, target)\n",
    "\n",
    "        mask_tr = g[time_col] <= cut_tr\n",
    "        mask_va = (g[time_col] > cut_tr) & (g[time_col] <= cut_va)\n",
    "        mask_te = g[time_col] > cut_va\n",
    "\n",
    "        if tag == \"L10\":\n",
    "            append_parquet(g.loc[mask_tr], Xtr_L10); append_parquet(g.loc[mask_va], Xva_L10); append_parquet(g.loc[mask_te], Xte_L10)\n",
    "            append_parquet(pd.DataFrame({target: g.loc[mask_tr, target].values}), ytr_L10)\n",
    "            append_parquet(pd.DataFrame({target: g.loc[mask_va, target].values}), yva_L10)\n",
    "            append_parquet(pd.DataFrame({target: g.loc[mask_te, target].values}), yte_L10)\n",
    "        else:\n",
    "            append_parquet(g.loc[mask_tr], Xtr_L20); append_parquet(g.loc[mask_va], Xva_L20); append_parquet(g.loc[mask_te], Xte_L20)\n",
    "            append_parquet(pd.DataFrame({target: g.loc[mask_tr, target].values}), ytr_L20)\n",
    "            append_parquet(pd.DataFrame({target: g.loc[mask_va, target].values}), yva_L20)\n",
    "            append_parquet(pd.DataFrame({target: g.loc[mask_te, target].values}), yte_L20)\n",
    "\n",
    "route_and_write(paths_L10, cut10_tr, cut10_va, \"L10\")\n",
    "route_and_write(paths_L20, cut20_tr, cut20_va, \"L20\")\n",
    "\n",
    "print(\"Per-line splits written (if available).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0528cb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\2553157763.py:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat(dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 2 → CoF_X_train.parquet (559866 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\2553157763.py:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat(dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 2 → CoF_X_val.parquet (69984 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\albert.dammara\\AppData\\Local\\Temp\\ipykernel_2720\\2553157763.py:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out = pd.concat(dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 2 → CoF_X_test.parquet (69984 rows)\n",
      "Merged 2 → CoF_y_train.parquet (559866 rows)\n",
      "Merged 2 → CoF_y_val.parquet (69984 rows)\n",
      "Merged 2 → CoF_y_test.parquet (69984 rows)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Merge per-line splits → combined\n",
    "def concat_splits(base_name):\n",
    "    files = list(PROCESSED_DIR.glob(f\"{base_name}_L*.parquet\"))\n",
    "    if not files:\n",
    "        return None\n",
    "    dfs = [pd.read_parquet(f) for f in files if f.exists()]\n",
    "    if not dfs:\n",
    "        return None\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "    out_path = PROCESSED_DIR / f\"{base_name}.parquet\"\n",
    "    out.to_parquet(out_path, index=False)\n",
    "    print(f\"Merged {len(dfs)} → {out_path.name} ({len(out)} rows)\")\n",
    "    return out_path\n",
    "\n",
    "for nm in [\"CoF_X_train\", \"CoF_X_val\", \"CoF_X_test\",\n",
    "           \"CoF_y_train\", \"CoF_y_val\", \"CoF_y_test\"]:\n",
    "    concat_splits(nm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1e903b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoF_y_train: positives=930 / 559866 (0.17%)\n",
      "CoF_y_val: positives=0 / 69984 (0.00%)\n",
      "CoF_y_test: positives=10260 / 69984 (14.66%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Diagnostics: positive counts per split\n",
    "for nm in [\"CoF_y_train\", \"CoF_y_val\", \"CoF_y_test\"]:\n",
    "    p = PROCESSED_DIR / f\"{nm}.parquet\"\n",
    "    if p.exists():\n",
    "        y = pd.read_parquet(p)\n",
    "        pos = int((y[target] == 1).sum())\n",
    "        total = len(y)\n",
    "        rate = (pos/total)*100 if total>0 else 0\n",
    "        print(f\"{nm}: positives={pos} / {total} ({rate:.2f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
